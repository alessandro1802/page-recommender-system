https://en.wikipedia.org/wiki/Haskins_Laboratories
Haskins Laboratories, Inc. is an independent 501(c) non-profit corporation,[4][5] founded in 1935 and located in New Haven, Connecticut, since 1970. Upon moving to New Haven, Haskins entered in to formal affiliation agreements with both Yale University and the University of Connecticut; it remains fully independent, administratively and financially, of both Yale and UConn. Haskins is a multidisciplinary and international community of researchers which conducts basic research on spoken and written language. A guiding perspective of their research is to view speech and language as emerging from biological processes, including those of adaptation, response to stimuli, and conspecific interaction. The Laboratories has a long history of technological and theoretical innovation, from creating systems of rules for speech synthesis and development of an early working prototype of a reading machine for the blind to developing the landmark concept of phonemic awareness as the critical preparation for learning to read an alphabetic writing system. Haskins Laboratories is equipped, in-house, with a comprehensive suite of tools and capabilities to advance its mission of research into language and literacy. As of 2014, these included: Many researchers have contributed to scientific breakthroughs at Haskins Laboratories since its founding. All of them are indebted to the pioneering work and leadership of Caryl Parker Haskins, Franklin S. Cooper, Alvin Liberman, Seymour Hutner and Luigi Provasoli. The history presented here focuses on the research program of the division of Haskins Laboratories that, since the 1940s, has been most well known for its work in the areas of speech, language, and reading.[6] Caryl Haskins and Franklin S. Cooper established Haskins Laboratories in 1935. It was originally affiliated with Harvard University, MIT, and Union College in Schenectady, NY. Caryl Haskins conducted research in microbiology, radiation physics, and other fields in Cambridge, MA and Schenectady. In 1939 the Laboratories moved its center to New York City. Seymour Hutner joined the staff to set up a research program in microbiology, genetics, and nutrition. The descendant of the division led by Hutner program eventually became a department of Pace University in New York.[1] The two identically named organizations are no longer formally affiliated. The U. S. Office of Scientific Research and Development, under Vannevar Bush asked Haskins Laboratories to evaluate and develop technologies for assisting blinded World War II veterans. Experimental psychologist Alvin Liberman joined the Laboratories to assist in developing a "sound alphabet" to represent the letters in a text for use in a reading machine for the blind. Luigi Provasoli joined the Laboratories to set up a research program in marine biology. The program in marine biology moved to Yale University in 1970 and disbanded with Provasoli's retirement in 1978. Franklin S. Cooper invented the pattern playback[2][3], a machine that converts pictures of the acoustic patterns of speech back into sound. With this device, Alvin Liberman, Cooper, and Pierre Delattre (and later joined by Katherine Safford Harris, Leigh Lisker, Arthur Abramson,  and others), discovered the acoustic cues for the perception of phonetic segments (consonants and vowels). Liberman and colleagues proposed a motor theory of speech perception to resolve the acoustic complexity: they hypothesized that we perceive speech by tapping into a biological specialization, a speech module, that contains knowledge of the acoustic consequences of articulation. Liberman, aided by Frances Ingemann and others, organized the results of the work on speech cues into a groundbreaking set of rules for speech synthesis by the Pattern Playback.[7] Franklin S. Cooper and Katherine Safford Harris, working with Peter MacNeilage, were the first researchers in the U.S. to use electromyographic techniques, pioneered at the University of Tokyo, to study the neuromuscular organization of speech. Leigh Lisker and Arthur Abramson looked for simplification at the level of articulatory action in the voicing of certain contrasting consonants. They showed that many acoustic properties of voicing contrasts arise from variations in voice onset time, the relative phasing of the onset of vocal cord vibration and the end of a consonant. Their work has been widely replicated and elaborated, here and abroad, over the following decades. Donald Shankweiler and Michael Studdert-Kennedy used a dichotic listening technique (presenting different nonsense syllables simultaneously to opposite ears) to demonstrate the dissociation of phonetic (speech) and auditory (nonspeech) perception by finding that phonetic structure devoid of meaning is an integral part of language, typically processed in the left cerebral hemisphere. Liberman, Cooper, Shankweiler, and Studdert-Kennedy summarized and interpreted fifteen years of research in "Perception of the Speech Code," still among the most cited papers in the speech literature. It set the agenda for many years of research at Haskins and elsewhere by describing speech as a code in which speakers overlap (or coarticulate) segments to form syllables. Researchers at Haskins connected their first computer to a speech synthesizer designed by the Laboratories' engineers. Ignatius Mattingly, with British collaborators, John N. Holmes [4] and J.N. Shearme [5], adapted the Pattern playback rules to write the first computer program for synthesizing continuous speech from a phonetically spelled input. A further step toward a reading machine for the blind combined Mattingly's program with an automatic look-up procedure for converting alphabetic text into strings of phonetic symbols. In 1970, Haskins Laboratories moved to New Haven, Connecticut, and entered into affiliation agreements with Yale University and the University of Connecticut; Haskins remains fully independent of both Yale and UConn, administratively and financially. The Lab's original location in New Haven, at 270 Crown Street (from 1970 to 2005), was leased from Yale University. Isabelle Liberman, Donald Shankweiler, and Alvin Liberman teamed up with Ignatius Mattingly to study the relationship between speech perception and reading, a topic implicit in the Laboratories' research program since its inception. They developed the concept of phonemic awareness, the knowledge that would-be readers must be aware of the phonemic structure of their language in order to be able to read. Leonard Katz related the work to contemporary cognitive theory and provided expertise in experimental design and data analysis. Under the broad rubric of the "alphabetic principle," this is the core of the Laboratories' present program of reading pedagogy. Patrick Nye [6] joined the Laboratories to lead a team working on the reading machine for the blind. The project culminated when the addition of an optical character recognizer allowed investigators to assemble the first automatic text-to-speech reading machine. By the end of the decade this technology had advanced to the point where commercial concerns assumed the task of designing and manufacturing reading machines for the blind [7]. In 1973, Franklin S. Cooper was selected to form a panel of six experts[8] charged with investigating the famous 18-minute gap in the White House office tapes of President Richard Nixon related to the Watergate scandal [8] Building on earlier work, Philip Rubin developed the sinewave synthesis program, which was then used by Robert Remez, Rubin,  and colleagues to show that listeners can perceive continuous speech without traditional speech cues from a pattern of sinewaves that track the changing resonances of the vocal tract. This paved the way for a view of speech as a dynamic pattern of trajectories through articulatory-acoustic space. Philip Rubin and colleagues developed Paul Mermelstein's anatomically simplified vocal tract model [9], originally worked on at Bell Laboratories, into the first articulatory synthesizer [10] that can be controlled in a physically meaningful way and used for interactive experiments. Studies of different writing systems supported the controversial hypothesis that all reading necessarily activates the phonological form of a word before, or at the same time, as its meaning. Work included experiments by Georgije Lukatela [11], Michael Turvey, Leonard Katz, Ram Frost, Laurie Feldman [12], and Shlomo Bentin, in a variety of languages. Cross-language work on reading, including investigations of the brain process involved, remains a large part of the Laboratories' program today. Various researchers developed compatible theoretical accounts of speech production,[9] speech perception and phonological knowledge. Carol Fowler proposed a direct realism theory of speech perception: listeners perceive gestures not by means of a specialized decoder, as in the motor theory, but because information in the acoustic signal specifies the gestures that form it. J. A. Scott Kelso and colleagues demonstrated functional synergies in speech gestures experimentally. Elliot Saltzman [13] developed a dynamical systems theory of synergetic action and implemented the theory as a working model of speech production. Linguists Catherine Browman and Louis Goldstein developed the theory of articulatory phonology [14], in which gestures are the basic units of both phonetic action and phonological knowledge. Articulatory phonology, the task dynamic model, and the articulatory synthesis model are combined into a gestural computational model of speech production [15]. Katherine Safford Harris,[10] Frederica Bell-Berti [16] and colleagues studied the phasing and cohesion of articulatory speech gestures. Kenneth Pugh was among the first scientists to use functional magnetic resonance imaging (fMRI) to reveal brain activity associated with reading and reading disabilities. Pugh, Donald Shankweiler, Weija Ni [17], Einar Mencl [18], and colleagues developed novel applications of neuroimaging to measure brain activity associated with understanding sentences. Philip Rubin, Louis Goldstein and Mark Tiede [19] designed a radical revision of the articulatory synthesis model, known as CASY [20], the configurable articulatory synthesizer. This 3-dimensional model of the vocal tract permits researchers to replicate MRI images of actual speakers. Douglas Whalen, Goldstein, Rubin and colleagues extended this work to study the relation between speech production and perception. [21] Donald Shankweiler, Susan Brady,  Anne Fowler [22], and others explored whether weak memory and perception in poor readers are tied specifically to phonological deficits. Evidence rejected broader cognitive deficits underlying reading difficulties and raised questions about impaired phonological representations in disabled readers. In 2000, Anne Fowler [23] and Susan Brady launched the Early Reading Success (ERS) program [24], part of the Haskins Literacy Initiative [25] which promotes the science of teaching reading. The ERS program was a demonstration project examining the efficacy of professional development in reading instruction for teachers of children in kindergarten through second grade. The Mastering Reading Instruction program [26], which combines professional development with Haskins-trained mentors, was a continuation of ERS. David Ostry and colleagues explored the neurological underpinning of motor control using a robot arm to influence jaw movement. Douglas Whalen and Khalil Iskarous [27] pioneered the pairing of ultrasound, used here to monitor articulators that cannot be seen, and Optotrak [28], an opto-electronic position-tracking device, used here to monitor visible articulators. Christine Shadle  [29] joined Haskins in 2004 to head up a project investigating the speech production goals for fricatives[30]. Donald Shankweiler and David Braze [31] developed an eye movement laboratory that combines eye tracking data with brain activity measures for investigating reading processes in normal and disabled readers. Laura Koenig and Jorge C. Lucero [32] studied the development of laryngeal and aerodynamic control in children's speech. In March 2005 Haskins Laboratories moved to a new, state-of-the-art facility on the 9th floor of a commercial building at 300 George Street in New Haven. This provides about 11,000 square feet of office and lab space. In 2008, Ken Pugh of Yale University was named President and Director of Research, succeeding Carol Fowler who remains at Haskins as a Senior Advisor. In 2009, Haskins released a new Strategic Plan [33], featuring new Birth-to-Five and Bilingualism initiatives. The Haskins Training Institute was established in 2011 to provide direct educational opportunities in Haskins Laboratories' core areas of research (language, speech perception, speech production, literacy).[11] The Training Institute serves to communicate this knowledge to the public through accessible seminars, small conferences, and intern and training positions. In December 2015, Haskins Laboratories convened a Global Literacy Summit.[12] This was a three-day meeting of scientists and representatives from governmental and non-governmental organizations around the globe, who are working with programs in the developing world to support literacy and education in disadvantaged populations. In 2016, Richard N. Aslin joined Haskins,[13] after leaving the University of Rochester.[14] In 2019, David Lewkowicz joined Haskins after leaving Northeastern University.[15] Nothing to report, yet.