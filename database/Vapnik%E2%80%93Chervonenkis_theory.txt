vapnik–chervonenkis theory also known theory developed vladimir vapnik alexey chervonenkis the theory form computational learning theory attempt explain learning process statistical point view theory related statistical learning theory empirical process richard dudley vladimir vapnik among others applied vc-theory empirical process theory cover least four part explained the nature statistical learning theory theory major subbranch statistical learning theory one main application statistical learning theory provide generalization condition learning algorithm from point view theory related stability alternative approach characterizing generalization addition theory dimension instrumental theory empirical process case process indexed class arguably important application theory employed proving generalization several technique introduced widely used empirical process theory the discussion mainly based book weak convergence empirical process with application statistic let \displaystyle \ldots independent identically distributed random element measurable space \displaystyle \mathcal \mathcal for measure \displaystyle \displaystyle \mathcal \mathcal measurable function \displaystyle \mathcal \to \mathbf define measurability issue ignored technical detail see let \displaystyle \mathcal class measurable function \displaystyle \mathcal \to \mathbf define define empirical measure stand dirac measure the empirical measure induces map \displaystyle \mathcal \to \mathbf given now suppose underlying true distribution data unknown empirical process theory aim identifying class \displaystyle \mathcal statement following hold \displaystyle \|\mathbb -p\|_ \mathcal \underset \to \displaystyle \left| \frac ... -\int fdp\right|\to former case \displaystyle \mathcal called glivenko–cantelli class latter case assumption sup \displaystyle \forall \sup \nolimits f\in \mathcal \vert -pf\vert \infty class \displaystyle \mathcal called donsker p-donsker donsker class glivenko–cantelli probability application slutsky theorem these statement true single \displaystyle standard lln clt argument regularity condition difficulty empirical process come joint statement made \displaystyle f\in \mathcal intuitively set \displaystyle \mathcal large turn geometry \displaystyle \mathcal play important role one way measuring big function set \displaystyle \mathcal use so-called covering number the covering number minimal number ball \displaystyle \|g-f\| \varepsilon needed cover set \displaystyle \mathcal obviously assumed underlying norm \displaystyle \mathcal the entropy logarithm covering number two sufficient condition provided proved set \displaystyle \mathcal glivenko–cantelli donsker class \displaystyle \mathcal p-glivenko–cantelli p-measurable envelope \displaystyle \ast \infty satisfies the next condition version celebrated dudley theorem \displaystyle \mathcal class function \displaystyle \mathcal p-donsker every probability measure \displaystyle \ast \infty last integral notation mean the majority argument bound empirical process rely symmetrization maximal concentration inequality chaining symmetrization usually first step proof since used many machine learning proof bounding empirical loss function including proof inequality discussed next section presented consider empirical process turn connection empirical following symmetrized process the symmetrized process rademacher process conditionally data \displaystyle therefore sub-gaussian process hoeffding inequality lemma symmetrization for every nondecreasing convex class measurable function \displaystyle \mathcal the proof symmetrization lemma relies introducing independent copy original variable \displaystyle sometimes referred ghost sample replacing inner expectation lh copy after application jensen inequality different sign could introduced hence name symmetrization without changing expectation the proof found instructive nature introduce ghost sample \displaystyle \ldots independent copy \displaystyle \ldots for fixed value \displaystyle \ldots one therefore jensen inequality taking expectation respect \displaystyle give note adding minus sign front term \displaystyle n't change rh symmetric function \displaystyle \displaystyle therefore rh remains sign perturbation \displaystyle \ldots \in therefore finally using first triangle inequality convexity \displaystyle \phi give where last two expression rh concludes proof typical way proving empirical clts first us symmetrization pas empirical process \displaystyle \mathbb argue conditionally data using fact rademacher process simple process nice property turn fascinating connection certain combinatorial property set \displaystyle \mathcal entropy number uniform covering number controlled notion vapnik–chervonenkis class set shortly set consider collection \displaystyle \mathcal subset sample space \displaystyle \mathcal \displaystyle \mathcal said pick certain subset \displaystyle finite set \displaystyle s=\ \ldots \subset \mathcal \displaystyle w=s\cap \displaystyle c\in \mathcal \displaystyle \mathcal said shatter pick subset the vc-index similar dimension appropriately chosen classifier set \displaystyle \mathcal \displaystyle \mathcal smallest set size shattered \displaystyle \mathcal sauer lemma state number \displaystyle \delta \mathcal \ldots subset picked vc-class \displaystyle \mathcal satisfies which polynomial number \displaystyle \mathcal subset rather exponential number intuitively mean finite vc-index implies \displaystyle \mathcal apparent simplistic structure similar bound shown different constant rate so-called subgraph class for function \displaystyle \mathcal \to \mathbf subgraph subset \displaystyle \mathcal \times \mathbf \displaystyle collection \displaystyle \mathcal called subgraph class subgraphs form vc-class consider set indicator function \displaystyle \mathcal \mathcal c\in \mathcal \displaystyle discrete empirical type measure equivalently probability measure shown quite remarkably \displaystyle r\geq further consider symmetric convex hull set \displaystyle \mathcal sconv \displaystyle \operatorname sconv \mathcal collection function form \displaystyle \sum \alpha \displaystyle \sum |\alpha |\leq then following valid convex hull \displaystyle \mathcal the important consequence fact enough entropy integral going converge therefore class sconv \displaystyle \operatorname sconv \mathcal going p-donsker finally example vc-subgraph class considered any finite-dimensional vector space \displaystyle \mathcal measurable function \displaystyle \mathcal \to \mathbf vc-subgraph index smaller equal dim \displaystyle \dim \mathcal proof take dim \displaystyle n=\dim \mathcal point \displaystyle \ldots the vector dimensional subspace take vector orthogonal subspace therefore consider set \displaystyle s=\ this set picked since \displaystyle \displaystyle s=\ would imply lh strictly positive rh non-positive there generalization notion subgraph class e.g notion pseudo-dimension the interested reader look similar setting considered common machine learning let \displaystyle \mathcal feature space \displaystyle \mathcal function \displaystyle \mathcal \to \mathcal called classifier let \displaystyle \mathcal set classifier similarly previous section define shattering coefficient also known growth function note function \displaystyle \mathcal set function thus define \displaystyle \mathcal collection subset obtained mapping every \displaystyle f\in \mathcal therefore term previous section shattering coefficient precisely this equivalence together sauer lemma implies \displaystyle \mathcal going polynomial sufficiently large provided collection \displaystyle \mathcal finite vc-index let \displaystyle \ldots observed dataset assume data generated unknown probability distribution \displaystyle define \displaystyle \neq expected loss course since \displaystyle unknown general one access \displaystyle however empirical risk given certainly evaluated then one following theorem for binary classification loss function following generalization bound word inequality saying sample increase provided \displaystyle \mathcal finite dimension empirical risk becomes good proxy expected risk note rh two inequality converge provided \displaystyle \mathcal grows polynomially the connection framework empirical process framework evident here one dealing modified empirical process surprisingly idea the proof first part inequality relies symmetrization argue conditionally data using concentration inequality particular hoeffding inequality the interested reader check book theorem