natural language processing nlp word embedding term used representation word text analysis typically form real-valued vector encodes meaning word word closer vector space expected similar meaning word embeddings obtained using set language modeling feature learning technique word phrase vocabulary mapped vector real number method generate mapping include neural network dimensionality reduction word co-occurrence matrix probabilistic model explainable knowledge base method explicit representation term context word appear word phrase embeddings used underlying input representation shown boost performance nlp task syntactic parsing sentiment analysis distributional semantics quantitative methodological approach understanding meaning observed language word embeddings semantic vector space model used knowledge representation time such model aim quantify categorize semantic similarity linguistic item based distributional property large sample language data the underlying idea word characterized company keep proposed article john rupert firth also root contemporaneous work search system cognitive psychology the notion semantic space lexical item word multi-word term represented vector embeddings based computational challenge capturing distributional characteristic using practical application measure similarity word phrase entire document the first generation semantic space model vector space model information retrieval such vector space model word distributional data implemented simplest form result sparse vector space high dimensionality curse dimensionality reducing number dimension using linear algebraic method singular value decomposition led introduction latent semantic analysis late random indexing approach collecting word cooccurrence context bengio provided series paper neural probabilistic language model reduce high dimensionality word representation context learning distributed representation word study published neurips nip introduced use word document embeddings applying method kernel cca bilingual multi-lingual corpus also providing early example self-supervised learning word embeddings word embeddings come two different style one word expressed vector co-occurring word another word expressed vector linguistic context word occur different style studied lavelli al. roweis saul published science use locally linear embedding lle discover representation high dimensional data structure most new word embedding technique rely neural network architecture instead probabilistic algebraic model since foundational work yoshua bengio colleague the approach adopted many research group advance around year made theoretical work quality vector training speed model hardware advance allowed broader parameter space explored profitably team google led tomas mikolov created word embedding toolkit train vector space model faster previous approach the approach widely used experimentation instrumental raising interest word embeddings technology moving research strand specialised research broader experimentation eventually paving way practical application historically one main limitation static word embeddings word vector space model word multiple meaning conflated single representation single vector semantic space word polysemy homonymy handled properly for example sentence the club tried yesterday great clear term club related word sense club sandwich baseball club clubhouse golf club sense club might the necessity accommodate multiple meaning per word different vector multi-sense embeddings motivation several contribution nlp split single-sense embeddings multi-sense one most approach produce multi-sense embeddings divided two main category word sense representation i.e. unsupervised knowledge-based based skip-gram multi-sense skip-gram mssg performs word-sense discrimination embedding simultaneously improving training time assuming specific number sens word non-parametric multi-sense skip-gram np-mssg number vary depending word combining prior knowledge lexical database e.g. wordnet conceptnet babelnet word embeddings word sense disambiguation most suitable sense annotation mssa label word-senses unsupervised knowledge-based approach considering word context pre-defined sliding window once word disambiguated used standard word embeddings technique multi-sense embeddings produced mssa architecture allows disambiguation annotation process performed recurrently self-improving manner the use multi-sense embeddings known improve performance several nlp task part-of-speech tagging semantic relation identification semantic relatedness named entity recognition sentiment analysis late contextually-meaningful embeddings elmo bert developed unlike static word embeddings embeddings token-level occurrence word embedding these embeddings better reflect multi-sense nature word occurrence word similar context situated similar region bert embedding space word embeddings n-grams biological sequence e.g dna rna protein bioinformatics application proposed asgari mofrad named bio-vectors biovec refer biological sequence general protein-vectors protvec protein amino-acid sequence gene-vectors genevec gene sequence representation widely used application deep learning proteomics genomics the result presented asgari mofrad suggest biovectors characterize biological sequence term biochemical biophysical interpretation underlying pattern word embeddings application game design proposed rabii cook way discover emergent gameplay using log gameplay data the process requires transcribe action happening game within formal language use resulting text create word embeddings the result presented rabii cook suggest resulting vector capture expert knowledge game like chess explicitly stated game rule the idea extended embeddings entire sentence even document e.g form thought vector concept researcher suggested skip-thought vector mean improve quality machine translation recent popular approach representing sentence sentence-bert sentencetransformers modifies pre-trained bert use siamese triplet network structure software training using word embeddings includes tomas mikolov stanford university glove gn-glove flair embeddings allennlp elmo bert fasttext gensim indra principal component analysis pca t-distributed stochastic neighbour embedding t-sne used reduce dimensionality word vector space visualize word embeddings cluster for instance fasttext also used calculate word embeddings text corpus sketch engine available online