https://en.wikipedia.org/wiki/Bernoulli_distribution
Three examples of Bernoulli distribution:     0 ≤ p ≤ 1   {\displaystyle 0\leq p\leq 1}   In probability theory and statistics, the Bernoulli distribution, named after Swiss mathematician Jacob Bernoulli,[1] is the discrete probability distribution of a random variable which takes the value 1 with probability     p   {\displaystyle p}   and the value 0 with probability     q = 1 − p   {\displaystyle q=1-p}  . Less formally, it can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes–no question. Such questions lead to outcomes that are boolean-valued: a single bit whose value is success/yes/true/one with probability p and failure/no/false/zero with probability q. It can be used to represent a (possibly biased) coin toss where 1 and 0 would represent "heads" and "tails", respectively, and p would be the probability of the coin landing on heads (or vice versa where 1 would represent tails and p would be the probability of tails).  In particular, unfair coins would have     p ≠ 1  /  2.   {\displaystyle p\neq 1/2.}   The Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted (so n would be 1 for such a binomial distribution). It is also a special case of the two-point distribution, for which the possible outcomes need not be 0 and 1. If     X   {\displaystyle X}   is a random variable with this distribution, then: The probability mass function     f   {\displaystyle f}   of this distribution, over possible outcomes k, is This can also be expressed as or as The Bernoulli distribution is a special case of the binomial distribution with     n = 1.   {\displaystyle n=1.}  [3] The kurtosis goes to infinity for high and low values of     p ,   {\displaystyle p,}   but for     p = 1  /  2   {\displaystyle p=1/2}   the two-point distributions including the Bernoulli distribution have a lower excess kurtosis than any other probability distribution, namely −2. The Bernoulli distributions for     0 ≤ p ≤ 1   {\displaystyle 0\leq p\leq 1}   form an exponential family. The maximum likelihood estimator of     p   {\displaystyle p}   based on a random sample is the sample mean. The expected value of a Bernoulli random variable     X   {\displaystyle X}   is This is due to the fact that for a Bernoulli distributed random variable     X   {\displaystyle X}   with     Pr ( X = 1 ) = p   {\displaystyle \Pr(X=1)=p}   and     Pr ( X = 0 ) = q   {\displaystyle \Pr(X=0)=q}   we find The variance of a Bernoulli distributed     X   {\displaystyle X}   is We first find From this follows With this result it is easy to prove that, for any Bernoulli distribution, its variance will have a value inside     [ 0 , 1  /  4 ]   {\displaystyle [0,1/4]}  . The skewness is        q − p   p q    =    1 − 2 p   p q      {\displaystyle {\frac {q-p}{\sqrt {pq}}}={\frac {1-2p}{\sqrt {pq}}}}  . When we take the standardized Bernoulli distributed random variable        X − E ⁡ [ X ]   Var ⁡ [ X ]      {\displaystyle {\frac {X-\operatorname {E} [X]}{\sqrt {\operatorname {Var} [X]}}}}   we find that this random variable attains       q  p q      {\displaystyle {\frac {q}{\sqrt {pq}}}}   with probability     p   {\displaystyle p}   and attains     −   p  p q      {\displaystyle -{\frac {p}{\sqrt {pq}}}}   with probability     q   {\displaystyle q}  . Thus we get The raw moments are all equal due to the fact that      1  k   = 1   {\displaystyle 1^{k}=1}   and      0  k   = 0   {\displaystyle 0^{k}=0}  .   The central moment of order     k   {\displaystyle k}   is given by The first six central moments are The higher central moments can be expressed more compactly in terms of      μ  2     {\displaystyle \mu _{2}}   and      μ  3     {\displaystyle \mu _{3}}   The first six cumulants are