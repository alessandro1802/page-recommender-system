computational learning theory probably approximately correct pac learning framework mathematical analysis machine learning proposed leslie valiant framework learner receives sample must select generalization function called hypothesis certain class possible function the goal high probability probably part selected function low generalization error approximately correct part the learner must able learn concept given arbitrary approximation ratio probability success distribution sample the model later extended treat noise misclassified sample important innovation pac framework introduction computational complexity theory concept machine learning particular learner expected find efficient function time space requirement bounded polynomial example size learner must implement efficient procedure requiring example count bounded polynomial concept size modified approximation likelihood bound order give definition something pac-learnable first introduce terminology for following definition two example used the first problem character recognition given array \displaystyle bit encoding binary-valued image the example problem finding interval correctly classify point within interval positive point outside range negative let \displaystyle set called instance space encoding sample character recognition problem instance space \displaystyle x=\ interval problem instance space \displaystyle set bounded interval \displaystyle \mathbb \displaystyle \mathbb denotes set real number concept subset \displaystyle c\subset one concept set pattern bit \displaystyle x=\ encode picture letter example concept second example set open interval \displaystyle \mid a\leq \pi \pi \leq b\leq \sqrt contains positive point concept class \displaystyle collection concept \displaystyle this could set subset array bit skeletonized width font let \displaystyle procedure draw example \displaystyle using probability distribution \displaystyle give correct label \displaystyle \displaystyle x\in otherwise now given \displaystyle \epsilon \delta assume algorithm \displaystyle polynomial \displaystyle \displaystyle relevant parameter class \displaystyle given sample size \displaystyle drawn according \displaystyle probability least \displaystyle \displaystyle output hypothesis \displaystyle h\in average error le equal \displaystyle \epsilon \displaystyle distribution \displaystyle further statement algorithm \displaystyle true every concept \displaystyle c\in every distribution \displaystyle \displaystyle \displaystyle \epsilon \delta \displaystyle efficiently pac learnable distribution-free pac learnable also say \displaystyle pac learning algorithm \displaystyle under regularity condition condition equivalent http //users.soe.ucsc.edu/~manfred/pubs/lrnk-olivier.pdf moran shay yehudayoff amir sample compression scheme class cs.lg