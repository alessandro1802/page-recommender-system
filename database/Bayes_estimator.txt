estimation theory decision theory bayes estimator bayes action estimator decision rule minimizes posterior expected value loss function i.e. posterior expected loss equivalently maximizes posterior expectation utility function alternative way formulating estimator within bayesian statistic maximum posteriori estimation suppose unknown parameter \displaystyle \theta known prior distribution \displaystyle \pi let \displaystyle \widehat \theta \widehat \theta estimator \displaystyle \theta based measurement let \displaystyle \theta \widehat \theta loss function squared error the bayes risk \displaystyle \widehat \theta defined \displaystyle \pi \theta \widehat \theta expectation taken probability distribution \displaystyle \theta defines risk function function \displaystyle \widehat \theta estimator \displaystyle \widehat \theta said bayes estimator minimizes bayes risk among estimator equivalently estimator minimizes posterior expected loss \displaystyle \theta \widehat \theta \displaystyle also minimizes bayes risk therefore bayes estimator prior improper estimator minimizes posterior expected loss \displaystyle called generalized bayes estimator the common risk function used bayesian estimation mean square error mse also called squared error risk the mse defined expectation taken joint distribution \displaystyle \theta \displaystyle using mse risk bayes estimate unknown parameter simply mean posterior distribution this known minimum mean square error mmse estimator inherent reason prefer one prior probability distribution another conjugate prior sometimes chosen simplicity conjugate prior defined prior distribution belonging parametric family resulting posterior distribution also belongs family this important property since bayes estimator well statistical property variance confidence interval etc derived posterior distribution conjugate prior especially useful sequential estimation posterior current measurement used prior next measurement sequential estimation unless conjugate prior used posterior distribution typically becomes complex added measurement bayes estimator usually calculated without resorting numerical method following example conjugate prior risk function chosen depending one measure distance estimate unknown parameter the mse common risk function use primarily due simplicity however alternative risk function also occasionally used the following several example alternative denote posterior generalized distribution function \displaystyle other loss function conceived although mean squared error widely used validated other loss function used statistic particularly robust statistic the prior distribution \displaystyle thus far assumed true probability distribution however occasionally restrictive requirement for example distribution covering set real number every real number equally likely yet sense distribution seems like natural choice non-informative prior i.e. prior distribution imply preference particular value unknown parameter one still define function \displaystyle \theta would proper probability distribution since infinite mass such measure \displaystyle \theta probability distribution referred improper prior the use improper prior mean bayes risk undefined since prior probability distribution take expectation consequence longer meaningful speak bayes estimator minimizes bayes risk nevertheless many case one define posterior distribution this definition application bayes theorem since bayes theorem applied distribution proper however uncommon resulting posterior valid probability distribution case posterior expected loss typically well-defined finite recall proper prior bayes estimator minimizes posterior expected loss when prior improper estimator minimizes posterior expected loss referred generalized bayes estimator typical example estimation location parameter loss function type \displaystyle a-\theta here \displaystyle \theta location parameter i.e. \displaystyle x|\theta x-\theta common use improper prior \displaystyle \theta case especially subjective information available this yield posterior expected loss the generalized bayes estimator value \displaystyle minimizes expression given \displaystyle this equivalent minimizing case shown generalized bayes estimator form \displaystyle x+a_ constant \displaystyle see let \displaystyle value minimizing \displaystyle then given different value \displaystyle must minimize this identical except \displaystyle replaced \displaystyle a-x_ thus expression minimizing given \displaystyle a-x_ =a_ optimal estimator form bayes estimator derived empirical bayes method called empirical bayes estimator empirical bayes method enable use auxiliary empirical data observation related parameter development bayes estimator this done assumption estimated parameter obtained common prior for example independent observation different parameter performed estimation performance particular parameter sometimes improved using data observation there parametric non-parametric approach empirical bayes estimation parametric empirical bayes usually preferable since applicable accurate small amount data the following simple example parametric empirical bayes estimation given past observation \displaystyle \ldots conditional distribution \displaystyle |\theta one interested estimating \displaystyle \theta based \displaystyle assume \displaystyle \theta common prior \displaystyle \pi depends unknown parameter for example suppose \displaystyle \pi normal unknown mean \displaystyle \mu \pi variance \displaystyle \sigma \pi use past observation determine mean variance \displaystyle \pi following way first estimate mean \displaystyle \mu variance \displaystyle \sigma marginal distribution \displaystyle \ldots using maximum likelihood approach next use law total expectation compute \displaystyle \mu law total variance compute \displaystyle \sigma \displaystyle \mu \theta \displaystyle \sigma \theta moment conditional distribution \displaystyle |\theta assumed known particular suppose \displaystyle \mu \theta =\theta \displaystyle \sigma \theta finally obtain estimated moment prior for example \displaystyle |\theta \sim \theta assume normal prior conjugate prior case conclude \displaystyle \theta \sim \widehat \mu \pi \widehat \sigma \pi bayes estimator \displaystyle \theta based \displaystyle calculated bayes rule finite bayes risk typically admissible the following specific example admissibility theorem contrast generalized bayes rule often undefined bayes risk case improper prior these rule often inadmissible verification admissibility difficult for example generalized bayes estimator location parameter based gaussian sample described generalized bayes estimator section inadmissible \displaystyle known stein phenomenon let unknown random variable suppose \displaystyle \ldots iid sample density \displaystyle |\theta let \displaystyle \delta =\delta \ldots sequence bayes estimator based increasing number measurement interested analyzing asymptotic performance sequence estimator i.e. performance \displaystyle \delta large end customary regard deterministic parameter whose true value \displaystyle \theta under specific condition large sample large value posterior density approximately normal word large effect prior probability posterior negligible moreover bayes estimator mse risk asymptotically unbiased converges distribution normal distribution fisher information follows bayes estimator mse asymptotically efficient another estimator asymptotically normal efficient maximum likelihood estimator mle the relation maximum likelihood bayes estimator shown following simple example consider estimator based binomial sample x~b denotes probability success assuming distributed according conjugate prior case beta distribution posterior distribution known a+x b+n-x thus bayes estimator mse the mle case x/n get the last equation implies bayes estimator described problem close mle hand small prior information still relevant decision problem affect estimate see relative weight prior information assume a=b case measurement brings new bit information formula show prior information weight a+b bit new information application one often know little fine detail prior distribution particular reason assume coincides exactly case one possible interpretation calculation non-pathological prior distribution mean value standard deviation give weight prior information equal bit new information another example phenomenon case prior estimate measurement normally distributed prior centered deviation measurement centered deviation posterior centered \displaystyle \frac \alpha \alpha +\beta \frac \beta \alpha +\beta weight weighted average moreover squared posterior deviation word prior combined measurement exactly way extra measurement take account for example deviation measurement combined match deviation prior assuming error measurement independent and weight formula posterior match weight prior time weight measurement combining prior measurement average result posterior centered \displaystyle \frac \frac particular prior play role measurement made advance general prior weight σ/σ measurement compare example binomial distribution prior weight σ/σ measurement one see exact weight depend detail distribution σ≫σ difference becomes small the internet movie database us formula calculating comparing rating film user including top rated title claimed give true bayesian estimate the following bayesian formula initially used calculate weighted average score top though formula since changed note weighted arithmetic mean weight vector number rating surpasses confidence average rating surpasses confidence mean vote film weighted bayesian rating approach straight average the closer number rating film zero closer weighted rating average rating film simpler term fewer ratings/votes cast film film weighted rating skew towards average across film film many ratings/votes rating approaching pure arithmetic average rating imdb approach ensures film rating would rank godfather example average rating