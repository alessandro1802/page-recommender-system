https://en.wikipedia.org/wiki/RDRAM
Rambus DRAM (RDRAM), and its successors Concurrent Rambus DRAM (CRDRAM) and Direct Rambus DRAM (DRDRAM), are types of synchronous dynamic random-access memory (SDRAM) developed by Rambus from the 1990s through to the early 2000s. The third-generation of Rambus DRAM, DRDRAM was replaced by XDR DRAM. Rambus DRAM was developed for high-bandwidth applications and was positioned by Rambus as replacement for various types of contemporary memories, such as SDRAM. DRDRAM was initially expected to become the standard in PC memory, especially after Intel agreed to license the Rambus technology for use with its future chipsets. Further, DRDRAM was expected to become a standard for graphics memory. However, RDRAM got embroiled in a standards war with an alternative technology—DDR SDRAM—and quickly lost out on grounds of price and, later, performance. By around 2003, DRDRAM was no longer supported in new personal computers. The first PC motherboards with support for RDRAM debuted in late 1999, after two major delays. RDRAM was controversial during its widespread use by Intel for having high licensing fees, high cost, being a proprietary standard, and low performance advantages for the increased cost. RDRAM and DDR SDRAM were involved in a standards war. PC-800 RDRAM operated at 400 MHz and delivered 1600 MB/s of bandwidth over a 16-bit bus.  It was packaged as a 184-pin RIMM (Rambus in-line memory module) form factor, similar to a DIMM (dual in-line memory module).  Data is transferred on both the rising and falling edges of the clock signal, a technique known as DDR. To emphasize the advantages of the DDR technique, this type of RAM was marketed at speeds twice the actual clock rate, i.e. the 400 MHz Rambus standard was named PC-800.  This was significantly faster than the previous standard, PC-133 SDRAM, which operated at 133 MHz and delivered 1066 MB/s of bandwidth over a 64-bit bus using a 168-pin DIMM form factor. Moreover, if a mainboard has a dual- or quad-channel memory subsystem, all of the memory channels must be upgraded simultaneously. 16-bit modules provide one channel of memory, while 32-bit modules provide two channels. Therefore, a dual-channel mainboard accepting 16-bit modules must have RIMMs added or removed in pairs. A dual-channel mainboard accepting 32-bit modules can have single RIMMs added or removed as well. Note that some of the later 32-bit modules had 232 pins as compared to the older 184-pin 16-bit modules.[1] The design of many common Rambus memory controllers dictated that memory modules be installed in sets of two. Any remaining open memory slots must be filled with continuity RIMMs (CRIMMs). These modules provide no extra memory and only served to propagate the signal to termination resistors on the motherboard instead of providing a dead end, where signals would reflect. CRIMMs appear physically similar to regular RIMMs, except that they lack integrated circuits (and their heat-spreaders). Compared to other contemporary standards, Rambus showed increase in latency, heat output, manufacturing complexity, and cost. Because of more complex interface circuitry and increased number of memory banks, RDRAM die size was larger than that of contemporary SDRAM chips, resulting in a 10–20% price premium at 16 Mbit densities (adding about a 5% penalty at 64 Mbit).[2] Note that the most common RDRAM densities are 128 Mbit and 256 Mbit. PC-800 RDRAM operated with a latency of 45 ns, more than that of other SDRAM varieties of the time. RDRAM memory chips also put out significantly more heat than SDRAM chips, necessitating heatspreaders on all RIMM devices. RDRAM includes additional circuitry (such as packet demultiplexers) on each chip, increasing manufacturing complexity compared to SDRAM. RDRAM was also up to four times more expensive than PC-133 SDRAM due to a combination of higher manufacturing costs and high license fees.[citation needed] PC-2100 DDR SDRAM, introduced in 2000, operated with a clock rate of 133 MHz and delivered 2100 MB/s over a 64-bit bus using a 184-pin DIMM form factor. With the introduction of the Intel 840 (Pentium III), Intel 850 (Pentium 4), Intel 860 (Pentium 4 Xeon) chipsets, Intel added support for dual-channel PC-800 RDRAM, doubling bandwidth to 3200 MB/s by increasing the bus width to 32 bits. This was followed in 2002 by the Intel 850E chipset, which introduced PC-1066 RDRAM, increasing total dual-channel bandwidth to 4200 MB/s. In 2002, Intel released the E7205 Granite Bay chipset, which introduced dual-channel DDR support (for a total bandwidth of 4200 MB/s) at a slightly lower latency than competing RDRAM. The bandwidth of Granite Bay matched that of the i850E chipset using PC-1066 DRDRAM with considerably lower latency. To achieve RDRAM's 800 MHz clock rate, the memory module runs on a 16-bit bus instead of a 64-bit bus in contemporary SDRAM DIMM. At the time of the Intel 820 launch some RDRAM modules operated at rates less than 800 MHz. Benchmark tests conducted in 1998 and 1999 showed most everyday applications to run minimally slower with RDRAM. In 1999, benchmarks comparing the Intel 840 and Intel 820 RDRAM chipsets with the Intel 440BX SDRAM chipset led to the conclusion that the performance gain of RDRAM did not justify its cost over SDRAM, except for use in workstations. In 2001, benchmarks pointed out that single-channel DDR266 SDRAM modules could closely match dual-channel 800 MHz RDRAM in everyday applications.[3] In November 1996, Rambus entered into a development and license contract with Intel.[4] Intel announced that it would only support the Rambus memory interface for its microprocessors[5] and had been granted rights to purchase one million shares of Rambus' stock at $10 per share.[6] As a transition strategy, Intel planned to support PC-100 SDRAM DIMMs on future Intel 82x chipsets using Memory Translation Hub (MTH).[7] In 2000, Intel recalled the Intel 820 motherboard, which featured the MTH, due to occasional occurrences of hanging and spontaneous reboots caused by simultaneous switching noise.[8] Since then, no production Intel 820 motherboards contain MTH. In 2000, Intel began to subsidize RDRAM by bundling retail boxes of Pentium 4s with two RIMMs.[9] Intel began to phase out these subsidies in 2001.[10] In 2003, Intel introduced the 865 and 875 chipsets with dual-channel DDR SDRAM support, which were marketed as high-end replacements of the 850 chipset. Furthermore, the future memory roadmap did not include RDRAM.[11] Rambus's RDRAM saw use in two video game consoles, beginning in 1996 with the Nintendo 64. The Nintendo console used 4 MB RDRAM running with a 500 MHz clock on a 9-bit bus, providing 500 MB/s bandwidth. RDRAM allowed N64 to be equipped with a large amount of memory bandwidth while maintaining a lower cost due to design simplicity. RDRAM's narrow bus allowed circuit board designers to use simpler design techniques to minimize cost. The memory, however, was disliked for its high random-access latencies. In the N64, the RDRAM modules are cooled by a passive heatspreader assembly.[12] Nintendo also included a provision for upgrading the system memory with the Expansion Pak accessory, allowing certain games to be enhanced with either enhanced graphics, higher resolution or increased framerate. A Jumper Pak dummy unit is included with the console due to the aforementioned design quirks of RDRAM. The Sony PlayStation 2 was equipped with 32 MB of RDRAM and implemented a dual-channel configuration resulting in 3200 MB/s available bandwidth. RDRAM was used in Texas Instruments' Digital Light Processing (DLP) systems.[13] Cirrus Logic implemented RDRAM support in their Laguna graphics chip, with two members of the family: the 2D-only 5462 and the 5464, a 2D chip with 3D acceleration. Both have 2 MB of memory and PCI port. Cirrus Logic GD5465 has extended 4 MB Rambus memory, dual-channel memory support and uses faster AGP port.[14] RDRAM offered a potentially faster user experience than competing DRAM technologies with its high bandwidth. The chips were used on the Creative Graphics Blaster MA3xx series, among others.